{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b93133e-5c48-4a69-9739-d82a4a7d34c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 09:02:31.158674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 09:02:31.956820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    " \n",
    "from transformers import (AutoTokenizer, Trainer, TrainingArguments,\n",
    "                          AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "                          LongformerConfig, LongformerForTokenClassification, BitsAndBytesConfig)\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import peft\n",
    "from peft import get_peft_config, get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "from seqeval.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57dfe9c3-0014-4407-80f5-1c5e392c4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280763a9-df43-44b6-a12a-aac7d1413c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e575fd0-52d0-401d-b7c7-50e120e42814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2eff20-0fae-4599-9e76-4fa22dd938f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace labels\n",
    "def replace_labels(row, labels):\n",
    "    \"\"\"\n",
    "    Replace labels in a row with 'O' if not in provided labels list.\n",
    "    \"\"\"\n",
    "    return [label if label in labels else 'O' for label in row]\n",
    "\n",
    "# Function to load data\n",
    "def load_data(labels):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from multiple sources.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    df_werner = pd.read_csv(f\"extra-data/individual_labels/email_new.csv\")\n",
    "    df_werner.rename(columns={'text': \"full_text\"}, inplace=True)\n",
    "    df_werner[\"tokens\"] = df_werner[\"tokens\"].apply(ast.literal_eval)\n",
    "    df_werner[\"trailing_whitespace\"] = df_werner[\"trailing_whitespace\"].apply(ast.literal_eval)\n",
    "    df_werner[\"labels\"] = df_werner[\"labels\"].apply(ast.literal_eval)\n",
    "\n",
    "    df_mixtral = pd.read_json(\"extra-data/mixtral-8x7b-v1.json\")\n",
    "\n",
    "    train_data = pd.read_json(\"pii-detection-removal-from-educational-data/train.json\")\n",
    "\n",
    "    gemma_df = pd.read_json(\"extra-data/pii_dataset_Gemma.json\")\n",
    "\n",
    "    df_mpware = json.load(open('extra-data/mpware_mixtral8x7b_v1.1-no-i-username.json'))\n",
    "    df_mpware = pd.DataFrame(df_mpware)\n",
    "    df_mpware = df_mpware[train_data.columns]\n",
    "\n",
    "    df_pj = pd.read_json('extra-data/moredata_dataset_fixed.json')\n",
    "\n",
    "    df_moth = pd.read_json('extra-data/pii_dataset_fixed.json')\n",
    "    df_moth.rename(columns={'text': \"full_text\"}, inplace=True)\n",
    "\n",
    "    # Combine dataframes\n",
    "    df = pd.concat([train_data, gemma_df, df_mpware, df_pj, df_moth, df_werner, df_mixtral])\n",
    "    df['document'] = [i for i in range(len(df))]  # Update the document\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['labels'] = df['labels'].apply(replace_labels, args=(labels,))\n",
    "\n",
    "    # Get unique labels\n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)), df['labels'].tolist())))\n",
    "\n",
    "    # Create label indexes\n",
    "    label2id = {label: index for index, label in enumerate(all_labels)}\n",
    "    id2label = {index: label for index, label in enumerate(all_labels)}\n",
    "    return df, all_labels, label2id, id2label\n",
    "\n",
    "# Function to encode labels\n",
    "def encode_labels(df):\n",
    "    \"\"\"\n",
    "    Encode labels to one-hot format.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal / total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes, subtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90cf8044-7886-4d21-ab7b-e5a7d17f7b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose appropriate target columns or all columns as required\n",
    "\n",
    "# target = [\n",
    "#     'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "#     'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "#     'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "# ]\n",
    "\n",
    "target = ['B-EMAIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f4cc79-b590-411b-bdda-d963f9ebfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_werner data = 755\n",
      "df_mixtral data = 2355\n",
      "kaggle train data = 6807\n",
      "gemma data =  5479\n",
      "df_mpware data = 2692\n",
      "df_pj data = 2000\n",
      "df_moth data = 4434\n",
      "all_labels = ['B-EMAIL', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset, labels and mapping dictionaries \n",
    "\n",
    "df, all_labels, label2id, id2label = load_data(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd25db4-8361-4e66-bac2-fefa7b15d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL: 9204  (37.5%)\n",
      "others: 15318  (62.5%)\n"
     ]
    }
   ],
   "source": [
    "df_labels, label_classes, true_size = encode_labels(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dacb7bd0-8060-4397-aa48-1b1498e40346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_df_by_sampling(df, n_samples, seed=None):\n",
    "    \"\"\"Split DataFrame into a sample and the remaining DataFrame.\"\"\"\n",
    "    samples_df = df.sample(n=n_samples, random_state=seed)\n",
    "    others_df = df.drop(df.index[samples_df.index], inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "def downsample_df(df, false_size):\n",
    "    \"\"\"Downsample DataFrame into training and validation datasets.\"\"\"\n",
    "    df['is_labels'] = df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n",
    "    \n",
    "    # Separate true and false labels\n",
    "    true_labels = df[df['is_labels']]\n",
    "    false_labels = df[~df['is_labels']] \n",
    "    \n",
    "    # Reset index\n",
    "    true_labels = true_labels.reset_index(drop=True)\n",
    "    false_labels = false_labels.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "    \n",
    "    # Calculate number of samples for validation set\n",
    "    n_samples_true = len(true_labels) - 150\n",
    "    \n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples_true, seed=42)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    \n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, false_size, seed=42)\n",
    "    false_others = false_others.sample(n=200, random_state=42)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    \n",
    "    # Training dataset = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])   \n",
    "    # Validation dataset = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])   \n",
    "    return train_df, valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb244a6-528e-4a4f-9746-891faa7d6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_size = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bd7685-f1a0-42d6-84e7-9d09af5c3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 9204\n",
      "Number of false_labels = 15318\n",
      "true_samples = 9054 true_others = 150\n",
      "false_samples = 12000 false_others = 200\n",
      "Number of train_df = 21054\n",
      "Number of valid_df = 350\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = downsample_df(df.copy(),false_size=false_size)\n",
    "\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# train_df = train_df.sample(100)\n",
    "# valid_df = valid_df.sample(100)\n",
    "\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a2ce3d-471e-404c-8228-8ef64ea9b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Tokenize input examples and map labels to their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        example (dict): Input example containing tokens, provided_labels, trailing_whitespace.\n",
    "        tokenizer: Tokenizer object.\n",
    "        label2id (dict): Mapping of labels to their corresponding IDs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized example with input_ids, attention_mask, offset_mapping, labels, and length.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store tokens and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over tokens, labels, and trailing whitespaces\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"provided_labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        # Repeat label for each character in token\n",
    "        labels.extend([label] * len(token))\n",
    "        # Add trailing whitespace and label if true\n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    # Concatenate tokens to form text\n",
    "    text = \"\".join(tokens)\n",
    "    \n",
    "    # Tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, max_length=4096, truncation=True)\n",
    "    \n",
    "    # Convert labels to numpy array\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Initialize list to store token labels\n",
    "    token_labels = []\n",
    "    \n",
    "    # Iterate over offset mappings\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Handle case when the text starts with whitespace\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])  # Add 'O' label\n",
    "        else:\n",
    "            try:\n",
    "                # If text starts with whitespace, adjust start index\n",
    "                if text[start_idx].isspace():\n",
    "                    start_idx += 1\n",
    "            except:\n",
    "                token_labels.append(label2id['O'])\n",
    "            # Convert label to ID\n",
    "            try:\n",
    "                label_id = label2id[labels[start_idx]]\n",
    "                token_labels.append(label_id)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Return tokenized example with labels and length\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f209a40-9239-4e24-9dd9-5022f8941de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, all_labels):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        preds (tuple): Tuple containing predictions and true labels.\n",
    "        all_labels (list): List of all possible labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictions, labels = preds\n",
    "        \n",
    "        # Remove ignored index (-100) from predictions and labels\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "        \n",
    "        # Compute recall, precision, and F1-score\n",
    "        recall = recall_score(true_labels, true_preds)\n",
    "        precision = precision_score(true_labels, true_preds)\n",
    "        \n",
    "        # Calculate modified F1-score\n",
    "        f1_score = (1 + 5**2) * recall * precision / ((5**2) * precision + recall)\n",
    "        \n",
    "        # Store metrics in a dictionary\n",
    "        result = {\n",
    "            'f1': f1_score,\n",
    "            'recall': recall,\n",
    "            'precision': precision\n",
    "        }\n",
    "        \n",
    "        # Print result for debugging or monitoring\n",
    "        print(f\"result = {result}\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions and print them for debugging\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1138bc0b-2e98-4fdf-b65c-f3e778a2950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        # Initialize with labels and paths\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.model_path = \"models/longformer_foundational/\"\n",
    "        self.save_path =  \"models/longformer_foundational_email/\"\n",
    "        self.num_proc = 5\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_train_epochs = 3\n",
    "        self.batch_size = 1\n",
    "        self.grad_steps = 16 \n",
    "        steps = len(train_df) // (self.batch_size * self.grad_steps) \n",
    "        \n",
    "        # Training arguments\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=\"./models\", \n",
    "            gradient_accumulation_steps=self.grad_steps,\n",
    "            fp16=True,\n",
    "            learning_rate=self.learning_rate,\n",
    "            num_train_epochs=self.num_train_epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            report_to=\"none\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=steps // 4,\n",
    "            do_eval=True,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=steps // 4,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=steps // 4,\n",
    "            lr_scheduler_type='linear',\n",
    "            load_best_model_at_end=False,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            warmup_ratio=0.1,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Load the model\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Create tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        config = LongformerConfig.from_pretrained(self.model_path)       \n",
    "        config.update({\n",
    "            'num_labels': len(self.all_labels),\n",
    "            'id2label': self.id2label,\n",
    "            'label2id': self.label2id,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        print(f\"Complete loading pretrained LLM model\") \n",
    "\n",
    "    def create_dataset(self, df):\n",
    "        # Create tokenized dataset\n",
    "        ds = Dataset.from_dict({\n",
    "            \"full_text\": df[\"full_text\"].tolist() ,\n",
    "            \"document\": df[\"document\"].astype('string'),\n",
    "            \"tokens\": df[\"tokens\"].tolist(),\n",
    "            \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "            \"provided_labels\": df[\"labels\"].tolist()\n",
    "        })\n",
    "        tokenized_ds = ds.map(\n",
    "            tokenize,\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"label2id\": self.label2id},\n",
    "            num_proc=self.num_proc\n",
    "        )\n",
    "        return tokenized_ds\n",
    "\n",
    "    def evaluate_saved_model(self, eval_df):\n",
    "        # Evaluate saved model\n",
    "        saved_model = AutoModelForTokenClassification.from_pretrained(self.save_path)\n",
    "        saved_tokenizer = AutoTokenizer.from_pretrained(self.save_path)\n",
    "        eval_ds = self.create_dataset(eval_df)\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer, pad_to_multiple_of=512)\n",
    "        trainer = Trainer(\n",
    "            model=saved_model,\n",
    "            args=self.training_args,\n",
    "            eval_dataset=eval_ds,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=saved_tokenizer,\n",
    "            compute_metrics=partial(compute_metrics, all_labels=self.all_labels),\n",
    "        )\n",
    "        evaluation_result = trainer.evaluate()\n",
    "        return evaluation_result\n",
    "        \n",
    "    def train(self, train_df, valid_df, resume=False):\n",
    "        # Train the model\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer, pad_to_multiple_of=512)\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=training_ds,\n",
    "            eval_dataset=valid_ds,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=partial(compute_metrics, all_labels=self.all_labels),\n",
    "        )\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fee514b-13fd-4af3-882e-e80a9d150db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at models/longformer_foundational/ and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([13]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([13, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading pretrained LLM model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3d4260702b406199d7e93a2579c2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/21054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00698c4674743629e3f8a96310a56ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3945' max='3945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3945/3945 3:35:11, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.999564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.989371</td>\n",
       "      <td>0.989207</td>\n",
       "      <td>0.993496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.999564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2296</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.999398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2624</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2952</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3608</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3936</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = {'f1': 0.9995643063134091, 'recall': 1.0, 'precision': 0.9887940234791889}\n",
      "result = {'f1': 0.9893709777870043, 'recall': 0.9892066918510524, 'precision': 0.9934959349593496}\n",
      "result = {'f1': 0.9995435684647302, 'recall': 1.0, 'precision': 0.9882666666666666}\n",
      "result = {'f1': 0.9994398921273727, 'recall': 1.0, 'precision': 0.9856382978723405}\n",
      "result = {'f1': 0.9994398921273727, 'recall': 1.0, 'precision': 0.9856382978723405}\n",
      "result = {'f1': 0.9995643063134091, 'recall': 1.0, 'precision': 0.9887940234791889}\n",
      "result = {'f1': 0.9993984276142469, 'recall': 1.0, 'precision': 0.9845908607863975}\n",
      "result = {'f1': 0.9997509856816768, 'recall': 1.0, 'precision': 0.9935656836461126}\n",
      "result = {'f1': 0.9997509856816768, 'recall': 1.0, 'precision': 0.9935656836461126}\n",
      "result = {'f1': 0.9997509856816768, 'recall': 1.0, 'precision': 0.9935656836461126}\n",
      "result = {'f1': 0.9997509856816768, 'recall': 1.0, 'precision': 0.9935656836461126}\n",
      "result = {'f1': 0.9997509856816768, 'recall': 1.0, 'precision': 0.9935656836461126}\n",
      "Save the model to models/longformer_foundational_email/\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "trainer.train(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cce426f-88f7-4166-a905-fe619ead5a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5682fb58d40341f1af25ac7c0125e518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = {'f1': 0.9997509856816768, 'recall': 1.0, 'precision': 0.9935656836461126}\n",
      "Evaluation result: {'eval_loss': 0.00029711303068324924, 'eval_f1': 0.9997509856816768, 'eval_recall': 1.0, 'eval_precision': 0.9935656836461126, 'eval_runtime': 18.5082, 'eval_samples_per_second': 18.911, 'eval_steps_per_second': 18.911}\n"
     ]
    }
   ],
   "source": [
    "evaluation_result = trainer.evaluate_saved_model(valid_df)\n",
    "print(\"Evaluation result:\", evaluation_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bhalewow-kernel",
   "language": "python",
   "name": "ritesh-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
