{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b93133e-5c48-4a69-9739-d82a4a7d34c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 09:36:36.023749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 09:36:36.764403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformer \n",
    "from transformers import (AutoTokenizer, Trainer, TrainingArguments,\n",
    "                          AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "                          LongformerConfig, LongformerForTokenClassification, BitsAndBytesConfig)\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import peft\n",
    "from peft import get_peft_config, get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57dfe9c3-0014-4407-80f5-1c5e392c4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280763a9-df43-44b6-a12a-aac7d1413c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e575fd0-52d0-401d-b7c7-50e120e42814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2eff20-0fae-4599-9e76-4fa22dd938f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def replace_labels(row, labels):\n",
    "    return [label if label in labels else 'O' for label in row]\n",
    "    \n",
    "\n",
    "def load_data(labels):\n",
    "    files = os.listdir(\"extra-data/individual_labels\")\n",
    "    csv_files = [f for f in files if f.endswith(\".csv\")]\n",
    "    \n",
    "    df_werner = pd.DataFrame()\n",
    "    \n",
    "    for file in csv_files:\n",
    "    \n",
    "        temp = pd.read_csv(f\"extra-data/individual_labels/{file}\")\n",
    "        temp.rename(columns={'text' : \"full_text\"}, inplace=True)\n",
    "        temp[\"tokens\"] = temp[\"tokens\"].apply(ast.literal_eval)\n",
    "        temp[\"trailing_whitespace\"] = temp[\"trailing_whitespace\"].apply(ast.literal_eval)\n",
    "        temp[\"labels\"] = temp[\"labels\"].apply(ast.literal_eval)\n",
    "        df_werner = pd.concat([df_werner,temp]).drop_duplicates(subset=['document'], keep='first')\n",
    "    \n",
    "    print(f\"df_werner data = {len(df_werner)}\")\n",
    "    \n",
    "\n",
    "    df_mixtral = pd.read_json(\"extra-data/mixtral-8x7b-v1.json\")\n",
    "    print(f\"df_mixtral data = {len(df_mixtral)}\")\n",
    "\n",
    "    \n",
    "    train_data = pd.read_json(\"pii-detection-removal-from-educational-data/train.json\")\n",
    "    print(f\"kaggle train data = {len(train_data)}\") # 6807\n",
    "    # Texts generated by Gemma\n",
    "    gemma_df = pd.read_json(\"extra-data/pii_dataset_Gemma.json\")\n",
    "    print(\"gemma data = \", len(gemma_df)) # 1390\n",
    "    # PII - Mixtral8x7B generated essays (2692)\n",
    "    df_mpware = json.load(open('extra-data/mpware_mixtral8x7b_v1.1-no-i-username.json'))\n",
    "    df_mpware = pd.DataFrame(df_mpware)    \n",
    "    df_mpware = df_mpware[train_data.columns]\n",
    "    print(f\"df_mpware data = {len(df_mpware)}\")\n",
    "\n",
    "    df_pj = pd.read_json('extra-data/moredata_dataset_fixed.json')\n",
    "    print(f\"df_pj data = {len(df_pj)}\")\n",
    "    df_pj.rename(columns={'text' : \"full_text\"}, inplace=True)\n",
    "    \n",
    "    df_moth = pd.read_json('extra-data/pii_dataset_fixed.json')\n",
    "    df_moth.rename(columns={'text' : \"full_text\"}, inplace=True)\n",
    "    print(f\"df_moth data = {len(df_moth)}\")\n",
    "    \n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, gemma_df, df_mpware, df_pj, df_moth, df_werner,df_mixtral])\n",
    "    df['document'] = [i for i in range(len(df))] # Update the document\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['labels'] = df['labels'].apply(replace_labels, args=(labels,))\n",
    "     # Get all the unique labels \n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)),\n",
    "                                                  df['labels'].tolist())))\n",
    "    print(f\"all_labels = {all_labels}\")\n",
    "    # Create indexes for labels\n",
    "    label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "    id2label = {index:label for index,label in enumerate(all_labels)}\n",
    "    return df, all_labels, label2id, id2label\n",
    "    \n",
    "# Encode labels to columns\n",
    "def encode_labels(df):\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "                                            list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    # add 'POS' column that don't have \n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal/total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes, subtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90cf8044-7886-4d21-ab7b-e5a7d17f7b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f4cc79-b590-411b-bdda-d963f9ebfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_werner data = 4117\n",
      "df_mixtral data = 2355\n",
      "kaggle train data = 6807\n",
      "gemma data =  5479\n",
      "df_mpware data = 2692\n",
      "df_pj data = 2000\n",
      "df_moth data = 4434\n",
      "all_labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n"
     ]
    }
   ],
   "source": [
    "df, all_labels, label2id, id2label = load_data(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd25db4-8361-4e66-bac2-fefa7b15d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL: 9204  (33.0%)\n",
      "ID_NUM: 5417  (19.4%)\n",
      "NAME_STUDENT: 14729  (52.8%)\n",
      "PHONE_NUM: 9489  (34.0%)\n",
      "STREET_ADDRESS: 11505  (41.3%)\n",
      "URL_PERSONAL: 5832  (20.9%)\n",
      "USERNAME: 6649  (23.8%)\n",
      "others: 7228  (25.9%)\n"
     ]
    }
   ],
   "source": [
    "df_labels, label_classes, true_size = encode_labels(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacb7bd0-8060-4397-aa48-1b1498e40346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df, n_samples):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=SEED)\n",
    "    # The remaining df\n",
    "    cond = df['document'].isin(samples_df['document'])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "def downsample_df(df,false_size):\n",
    "    '''Split the df into training and valid dataset'''\n",
    "    df['is_labels'] = df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df['is_labels'] == True]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[df['is_labels'] == False] \n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "    # Get 300 as valid dataset\n",
    "    n_samples=len(true_labels) - 50\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples=false_size\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    false_others = false_others.sample(n = 200)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])   \n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])   \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb244a6-528e-4a4f-9746-891faa7d6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_size = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bd7685-f1a0-42d6-84e7-9d09af5c3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 20656\n",
      "Number of false_labels = 7228\n",
      "true_samples = 20606 true_others = 50\n",
      "false_samples = 7000 false_others = 200\n",
      "Number of train_df = 27606\n",
      "Number of valid_df = 250\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = downsample_df(df.copy(),false_size=false_size)\n",
    "\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# train_df = train_df.sample(100)\n",
    "# valid_df = valid_df.sample(100)\n",
    "\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a2ce3d-471e-404c-8228-8ef64ea9b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"provided_labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, max_length = 4096,\n",
    "                          truncation=True)\n",
    "    labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"]) \n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            try:\n",
    "                if text[start_idx].isspace():\n",
    "                    start_idx += 1\n",
    "            except:\n",
    "                token_labels.append(label2id['O'])\n",
    "            # Convert label to id (int)\n",
    "            try:\n",
    "                label_id = label2id[labels[start_idx]]\n",
    "                token_labels.append(label_id)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9f144a-d11b-41ed-bc05-680f73fefdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f209a40-9239-4e24-9dd9-5022f8941de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, all_labels):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(true_labels, true_preds)\n",
    "        precision = precision_score(true_labels, true_preds)\n",
    "        # Use modified f1 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1138bc0b-2e98-4fdf-b65c-f3e778a2950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        \n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        \n",
    "        self.model_path = \"allenai/longformer-base-4096\"\n",
    "        self.save_path =  \"models/longformer_foundational_all\"\n",
    "        self.num_proc = 5\n",
    "        \n",
    "        self.learning_rate = 1e-5\n",
    "        self.num_train_epochs = 3 # Number of epochs\n",
    "        self.batch_size = 2\n",
    "        self.load_model()\n",
    "        # self.grad_steps = 16 \n",
    "        steps = len(train_df) // self.batch_size \n",
    "        \n",
    "        self.training_args = TrainingArguments(output_dir=\"./models\", \n",
    "                                          # gradient_accumulation_steps=self.grad_steps,\n",
    "                                          fp16=True,\n",
    "                                          learning_rate=self.learning_rate,\n",
    "                                          num_train_epochs=self.num_train_epochs, # The total number of training epochs to run.\n",
    "                                          per_device_train_batch_size=self.batch_size,  # batch size per device during training\n",
    "                                          per_device_eval_batch_size=self.batch_size, # batch size for evaluation\n",
    "                                          report_to=\"none\",\n",
    "                                          evaluation_strategy=\"steps\", # Evaluated at the end of epochs\n",
    "                                          eval_steps=steps // 2,\n",
    "                                          do_eval=True,\n",
    "                                          save_strategy=\"steps\",\n",
    "                                          save_steps=steps // 2,\n",
    "                                          save_total_limit=2, # Save the best and most recent checkpoints\n",
    "                                          logging_steps=steps // 2,\n",
    "                                          lr_scheduler_type='cosine',\n",
    "                                          load_best_model_at_end=True, # Load the best model at the end\n",
    "                                          metric_for_best_model=\"f1\",\n",
    "                                          greater_is_better=True,\n",
    "                                          warmup_ratio = 0.05, \n",
    "                                          weight_decay=0.001, \n",
    "                                         )\n",
    "\n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Load tokenizer config\n",
    "        config = LongformerConfig.from_pretrained(self.model_path)       \n",
    "        # Increase context length using the max_position_embeddings parameter \n",
    "        config.update({\n",
    "            'num_labels': len(self.all_labels),\n",
    "            'id2label': self.id2label,\n",
    "            'label2id': self.label2id,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "            'layer_norm_eps' : 1e-7,\n",
    "        })\n",
    "\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path,\n",
    "                                                                     config=config, ignore_mismatched_sizes = True)\n",
    "        \n",
    "        print(f\"Complete loading pretrained LLM model\") \n",
    "\n",
    "    # Convert df to tokenized dataset\n",
    "    def create_dataset(self, df):\n",
    "        ds = Dataset.from_dict({\n",
    "            \"full_text\": df[\"full_text\"].tolist() ,\n",
    "            \"document\": df[\"document\"].astype('string'),\n",
    "            \"tokens\": df[\"tokens\"].tolist(),\n",
    "            \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "            \"provided_labels\": df[\"labels\"].tolist()\n",
    "        })\n",
    "         # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(tokenize, \n",
    "                              fn_kwargs={\"tokenizer\": self.tokenizer, \n",
    "                                         \"label2id\": self.label2id,\n",
    "                                        },\n",
    "                              num_proc=self.num_proc)\n",
    "        return tokenized_ds\n",
    "\n",
    "    # Evaluate the saved model\n",
    "    def evaluate_saved_model(self, eval_df):\n",
    "        # Load the saved model and tokenizer\n",
    "        saved_model = AutoModelForTokenClassification.from_pretrained(self.save_path)\n",
    "        saved_tokenizer = AutoTokenizer.from_pretrained(self.save_path)\n",
    "\n",
    "        # Create dataset for evaluation\n",
    "        eval_ds = self.create_dataset(eval_df)\n",
    "\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer, pad_to_multiple_of=512)\n",
    "\n",
    "        # Evaluate the model\n",
    "        trainer = Trainer(model=saved_model,\n",
    "                          args=self.training_args,\n",
    "                          eval_dataset=eval_ds,\n",
    "                          data_collator=data_collator,\n",
    "                          tokenizer=saved_tokenizer,\n",
    "                          compute_metrics=partial(compute_metrics, all_labels=self.all_labels),\n",
    "                          )\n",
    "\n",
    "        evaluation_result = trainer.evaluate()\n",
    "\n",
    "        return evaluation_result\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, train_df, valid_df,resume=False):\n",
    "        # Create training dataset\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        # Create valid dataset\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer, pad_to_multiple_of=512)               \n",
    "        # Trainer cofiguration\n",
    "        \n",
    "        # Pass the modelTrainer\n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=self.training_args, \n",
    "                          train_dataset=training_ds,\n",
    "                          eval_dataset=valid_ds, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer,\n",
    "                          compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    "                         )\n",
    "        # Train the model\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "        # Save the model\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee514b-13fd-4af3-882e-e80a9d150db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading pretrained LLM model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b47c8692a4704a6c1f3017c2c2e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/27606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc3baeae0d542cca52e6a7f3d528996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13396' max='41409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13396/41409 52:24 < 3:46:06, 2.06 it/s, Epoch 0.97/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "trainer.train(train_df, valid_df,resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce426f-88f7-4166-a905-fe619ead5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_result = trainer.evaluate_saved_model(valid_df)\n",
    "# print(\"Evaluation result:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bdf4c-b802-4e06-bacc-d1b0da6ca229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bhalewow-kernel",
   "language": "python",
   "name": "ritesh-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
