{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b93133e-5c48-4a69-9739-d82a4a7d34c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pytorch/Ritesh-D11AD-05/Ritesh_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformer \n",
    "from transformers import (AutoTokenizer, Trainer, TrainingArguments,\n",
    "                          AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "                          DebertaV2Config, DebertaV2ForTokenClassification)\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57dfe9c3-0014-4407-80f5-1c5e392c4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280763a9-df43-44b6-a12a-aac7d1413c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e575fd0-52d0-401d-b7c7-50e120e42814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2eff20-0fae-4599-9e76-4fa22dd938f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def replace_labels(row, labels):\n",
    "    return [label if label in labels else 'O' for label in row]\n",
    "    \n",
    "\n",
    "def load_data(labels):\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(\"pii-detection-removal-from-educational-data/train.json\")\n",
    "    print(f\"kaggle train data = {len(train_data)}\") # 6807\n",
    "    # Texts generated by Gemma\n",
    "    gemma_df = pd.read_json(\"extra-data/pii_dataset_Gemma.json\")\n",
    "    print(\"gemma data = \", len(gemma_df)) # 1390\n",
    "    # PII - Mixtral8x7B generated essays (2692)\n",
    "    df_mpware = json.load(open('extra-data/mpware_mixtral8x7b_v1.1-no-i-username.json'))\n",
    "    df_mpware = pd.DataFrame(df_mpware)    \n",
    "    df_mpware = df_mpware[train_data.columns]\n",
    "    print(f\"df_mpware data = {len(df_mpware)}\")\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, gemma_df, df_mpware])\n",
    "    df['document'] = [i for i in range(len(df))] # Update the document\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['labels'] = df['labels'].apply(replace_labels, args=(labels,))\n",
    "     # Get all the unique labels \n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)),\n",
    "                                                  df['labels'].tolist())))\n",
    "    print(f\"all_labels = {all_labels}\")\n",
    "    # Create indexes for labels\n",
    "    label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "    id2label = {index:label for index,label in enumerate(all_labels)}\n",
    "    return df, all_labels, label2id, id2label\n",
    "    \n",
    "# Encode labels to columns\n",
    "def encode_labels(df):\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "                                            list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    # add 'POS' column that don't have \n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal/total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f4cc79-b590-411b-bdda-d963f9ebfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "gemma data =  5479\n",
      "df_mpware data = 2692\n",
      "all_labels = ['B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O']\n"
     ]
    }
   ],
   "source": [
    "df, all_labels, label2id, id2label = load_data(['B-STREET_ADDRESS','I-STREET_ADDRESS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd25db4-8361-4e66-bac2-fefa7b15d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREET_ADDRESS: 4031  (26.9%)\n",
      "others: 10947  (73.1%)\n"
     ]
    }
   ],
   "source": [
    "df_labels, label_classes = encode_labels(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dacb7bd0-8060-4397-aa48-1b1498e40346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df, n_samples):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=SEED)\n",
    "    # The remaining df\n",
    "    cond = df['document'].isin(samples_df['document'])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "def downsample_df(df,false_size):\n",
    "    '''Split the df into training and valid dataset'''\n",
    "    df['is_labels'] = df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df['is_labels'] == True]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[df['is_labels'] == False] \n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "    # Get 300 as valid dataset\n",
    "    n_samples=len(true_labels) - 50\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples=false_size\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    false_others = false_others.sample(n = 200)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])   \n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])   \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29bd7685-f1a0-42d6-84e7-9d09af5c3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 4031\n",
      "Number of false_labels = 10947\n",
      "true_samples = 3981 true_others = 50\n",
      "false_samples = 6000 false_others = 200\n",
      "Number of train_df = 9981\n",
      "Number of valid_df = 250\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = downsample_df(df.copy(),false_size=6000)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a2ce3d-471e-404c-8228-8ef64ea9b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"provided_labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"]) \n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            label_id = label2id[labels[start_idx]]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9f144a-d11b-41ed-bc05-680f73fefdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f209a40-9239-4e24-9dd9-5022f8941de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, all_labels):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(true_labels, true_preds)\n",
    "        precision = precision_score(true_labels, true_preds)\n",
    "        # Use modified f1 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1138bc0b-2e98-4fdf-b65c-f3e778a2950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.num_proc = 3\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_train_epochs = 3 # Number of epochs\n",
    "        self.batch_size = 1 # Default (4) Too large batch sizes lead to OOM\n",
    "        self.fp16 = True if torch.cuda.is_available() else False\n",
    "        self.model_path = \"microsoft/deberta-v3-small\"\n",
    "        self.output_dir = \"outputs/\"\n",
    "        self.save_path =  \"models/deberta3small_address_model_512\"\n",
    "        self.load_model()\n",
    "        \n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Load tokenizer config\n",
    "        config = DebertaV2Config.from_pretrained(self.model_path)       \n",
    "        # Increase context length using the max_position_embeddings parameter \n",
    "        config.update({\n",
    "            'num_labels': len(self.all_labels),\n",
    "            'id2label': self.id2label,\n",
    "            'label2id': self.label2id,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path,\n",
    "                                                                     config=config)\n",
    "        print(f\"Complete loading pretrained LLM model\") \n",
    "        \n",
    "    # Convert df to tokenized dataset\n",
    "    def create_dataset(self, df):\n",
    "        ds = Dataset.from_dict({\n",
    "            \"full_text\": df[\"full_text\"].tolist() ,\n",
    "            \"document\": df[\"document\"].astype('string'),\n",
    "            \"tokens\": df[\"tokens\"].tolist(),\n",
    "            \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "            \"provided_labels\": df[\"labels\"].tolist()\n",
    "        })\n",
    "         # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(tokenize, \n",
    "                              fn_kwargs={\"tokenizer\": self.tokenizer, \n",
    "                                         \"label2id\": self.label2id},\n",
    "                              num_proc=self.num_proc)\n",
    "        return tokenized_ds\n",
    "    \n",
    "    # Train the model\n",
    "    def train(self, train_df, valid_df):       \n",
    "        # Create training dataset\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        # Create valid dataset\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer, pad_to_multiple_of=16)               \n",
    "        # Trainer cofiguration\n",
    "        training_args = TrainingArguments(output_dir=self.output_dir, \n",
    "                                          fp16=self.fp16, # # Change to False if using CPU only\n",
    "                                          learning_rate=self.learning_rate,\n",
    "                                          num_train_epochs=self.num_train_epochs, # The total number of training epochs to run.\n",
    "                                          per_device_train_batch_size=self.batch_size,  # batch size per device during training\n",
    "                                          per_device_eval_batch_size=self.batch_size, # batch size for evaluation\n",
    "                                          gradient_accumulation_steps=2, \n",
    "                                          report_to=\"none\",\n",
    "                                          evaluation_strategy=\"epoch\", # Evaluated at the end of epochs\n",
    "                                          # eval_steps=1,\n",
    "                                          do_eval=True,\n",
    "                                          save_strategy=\"epoch\",\n",
    "                                          save_total_limit=2, # Save the best and most recent checkpoints\n",
    "                                          logging_steps=20,\n",
    "                                          lr_scheduler_type='cosine',\n",
    "                                          load_best_model_at_end=True, # Load the best model at the end\n",
    "                                          metric_for_best_model=\"f1\",\n",
    "                                          greater_is_better=True,\n",
    "                                          warmup_ratio=0.1, # number of warmup steps (0.1) for learning rate scheduler\n",
    "                                          weight_decay=0.01, # strength of weight decay\n",
    "                                         )\n",
    "        # Pass the modelTrainer\n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=training_args, \n",
    "                          train_dataset=training_ds,\n",
    "                          eval_dataset=valid_ds, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer,\n",
    "                          compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    "                         )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Save the model\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee514b-13fd-4af3-882e-e80a9d150db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pytorch/Ritesh-D11AD-05/Ritesh_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading pretrained LLM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|█████████████████████████████████████████████████████| 9981/9981 [00:26<00:00, 378.02 examples/s]\n",
      "Map (num_proc=3): 100%|███████████████████████████████████████████████████████| 250/250 [00:01<00:00, 141.39 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='14970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/14970 38:40 < 13:39, 4.77 it/s, Epoch 2.22/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.925532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.947196</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.956989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = {'f1': 0.925531914893617, 'recall': 0.925531914893617, 'precision': 0.925531914893617}\n",
      "result = {'f1': 0.9471960704052393, 'recall': 0.9468085106382979, 'precision': 0.956989247311828}\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "trainer.train(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6a79b-88e6-4848-bd69-4e905874b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1631f-41e3-42a3-8598-65c9dd9eacc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ritesh-kernel",
   "language": "python",
   "name": "ritesh-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
